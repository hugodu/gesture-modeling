{"name":"Gesture-modeling","tagline":"Creating Models for Learning and Recognizing Multitouch Gestures.","body":"This project is aimed at creating a language / framework independent Gesture\r\nRecognition toolkit that takes OSC messages formatted with TUIO specification\r\nas input and outputs recognized gestures via OSC protocol. The problem with\r\ngestures such as the pinch-to-zoom, rotate are hard-coded and require more coding\r\nto add new gestures into the system. Apart from this, there are toolkits that go one\r\nstep ahead to learn and recognize single point stroke gestures based on their shape,\r\nlike the nui wave to bring up a menu. Although this is very useful for application\r\ndevelopers to use, it is still limited by not using multiple points of input that a\r\nmultitouch surface allows.\r\nThis project will focus on gestures of two types, non-parameterized gestures and\r\nparameterized gestures. Non-parameterized gestures, as the name suggests, map to\r\nactions that do not require any quantified parameters for their corresponding action,\r\nsuch as a \"switch to fullscreen\" or \"Delete element\". Parameterized gestures, on\r\nthe other hand, once recognized will follow with a stream of live parameters, such\r\nas a \"Change hue(/saturation/value/alpha) of an element\" allowing control over\r\nparameters of that action. Recognized gestures will be outputed as OSC messages\r\nwith optional parameters (such as x,y coords of where the gestures is performed),\r\nso applications can contextualize the operation of the gesture.\r\n<br/><br/>\r\nThis project will build an architecture for future multitouch applications to use\r\nnatural gestures that are decided by the application developer, and can be learnt with\r\nminimal training samples. The tools and communication channels used are crossplatform,\r\nlanguage independant. Much like the TUIO protocol, all that is required\r\nby any framework to use this will be a simple wrapper written in the language of\r\nthe framework. Using this augmented communication protocol, applications will\r\nregister elements as GestureListeners, and also register the context of interaction -\r\nthe set of gestures that are currently available for the user to perform.\r\nThe project I am proposing will draw from machine learning literature to use\r\nbayesian networks to model learnt gestures. Such techniques allow multitouch\r\napplication developers to use complex gestures by simply teaching them by a few\r\nexamples. I have previously demonstrated the feasability of such techniques using\r\nHidden Markov Models to successfully learn and subsequently recognize gestures\r\nthat have multiple points of contact (http://tinyurl.com/mtGestureLearning).\r\nI will be using AMELiA (http://ame4.hc.asu.edu/amelia/) as the gesture recognition\r\ntoolkit for this project. AMELiA is a generic gesture recognition toolkit that\r\nallows developers to build domain specific models for learning and recognizing\r\ngestures. I will be using this toolkit to build models specifically for multitouch\r\ninput. The probabilistic bayesian models availabe within the AMELiA toolkit are\r\nappropriate for creating Interaction Contexts with a set of gestures that the application\r\nsupports. As the library includes the training and recognition algorithms, my\r\nproject will only require implementing the domain specific code for the modeling\r\nmultitouch data. This makes the project tractable within the timeline of summer\r\nof code.\r\n<br/><br/>\r\nThis project takes it’s inspiration from the gestures developed byWayneWesterman\r\nduring for his PhD thesis that evolved into the company FingerWorks and\r\nare now in Apple’s new multitouch trackpads. One of the key difference to note between\r\nmy proposed project and Westerman’s work is that a multitouch input-only\r\ndevice was used to develop gestures for his devices, whereas today we are working\r\nwith interactive screens, that merge display and input into the same space. To use\r\ngestures affectively, we will need to experiment with multiple mappings for operations\r\nwith different gestures. This project develops the framework for interaction\r\ndesigners to easily create and use multitouch gestures within their application.\r\nOrientation: The orientation of a multitouch gesture for a recognition algorithm\r\nis tied to the problem of dimensionality. Since touchlib/tBeta assigns id’s\r\nin increasing order to contacts, multiple fingers may have varying order across\r\nsamples. For example, consider a three finger gesture with the thumb, index and\r\nmiddle finger. During the first sample, the fingers may have id’s (0,1,2) whereas\r\nthe second time the gesture is performed their id’s might be (1,0,2) respectively.\r\nThis is a problem for the recognizer. A heuristic that has worked to solve this\r\nproblem uses the orientation of each finger to consistently re-assign id’s to the\r\ncontacts across samples. I will apply a shape matching heuristic that can quickly\r\nre-assign id’s by matching the shape of the contacts. This will make all samples\r\nof a gesture have consistent id ordering, and allow the gestures to be recognized\r\nindependent of the orientation.\r\n<br/>\r\nDevelopment Methodologies:\r\nMy proposed project will be developed in a C++ makefile environment that can\r\ncompile cross-platform. This project will be developed through a series of iterations,\r\nwith a concrete deliverable at every stage.\r\n<br/><br/>\r\nOriginally hosted [here](http://nuicode.com/projects/gsoc-gesture-models)\r\n<br/>\r\n***\r\nGSoC 2009 project\r\n<br/>\r\n[![GSoC 2009](http://db.tt/Uai2WuQE)](http://www.google-melange.com/gsoc/homepage/google/gsoc2009)\r\n<br/>\r\n**Student:** Sashikanth Damaraju\r\n<br/>\r\n**Mentor:** Stjepan Rajko","google":"UA-38848919-1","note":"Don't delete this file! It's used internally to help with page regeneration."}