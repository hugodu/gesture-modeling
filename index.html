<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Gesture-modeling by nuigroup</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Gesture-modeling</h1>
        <p>Creating Models for Learning and Recognizing Multitouch Gestures.</p>

        <p class="view"><a href="https://github.com/nuigroup/gesture-modeling">View the Project on GitHub <small>nuigroup/gesture-modeling</small></a></p>


        <ul>
          <li><a href="https://github.com/nuigroup/gesture-modeling/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/nuigroup/gesture-modeling/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/nuigroup/gesture-modeling">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <p>This project is aimed at creating a language / framework independent Gesture
Recognition toolkit that takes OSC messages formatted with TUIO specification
as input and outputs recognized gestures via OSC protocol. The problem with
gestures such as the pinch-to-zoom, rotate are hard-coded and require more coding
to add new gestures into the system. Apart from this, there are toolkits that go one
step ahead to learn and recognize single point stroke gestures based on their shape,
like the nui wave to bring up a menu. Although this is very useful for application
developers to use, it is still limited by not using multiple points of input that a
multitouch surface allows.
This project will focus on gestures of two types, non-parameterized gestures and
parameterized gestures. Non-parameterized gestures, as the name suggests, map to
actions that do not require any quantified parameters for their corresponding action,
such as a "switch to fullscreen" or "Delete element". Parameterized gestures, on
the other hand, once recognized will follow with a stream of live parameters, such
as a "Change hue(/saturation/value/alpha) of an element" allowing control over
parameters of that action. Recognized gestures will be outputed as OSC messages
with optional parameters (such as x,y coords of where the gestures is performed),
so applications can contextualize the operation of the gesture.
<br><br>
This project will build an architecture for future multitouch applications to use
natural gestures that are decided by the application developer, and can be learnt with
minimal training samples. The tools and communication channels used are crossplatform,
language independant. Much like the TUIO protocol, all that is required
by any framework to use this will be a simple wrapper written in the language of
the framework. Using this augmented communication protocol, applications will
register elements as GestureListeners, and also register the context of interaction -
the set of gestures that are currently available for the user to perform.
The project I am proposing will draw from machine learning literature to use
bayesian networks to model learnt gestures. Such techniques allow multitouch
application developers to use complex gestures by simply teaching them by a few
examples. I have previously demonstrated the feasability of such techniques using
Hidden Markov Models to successfully learn and subsequently recognize gestures
that have multiple points of contact (<a href="http://tinyurl.com/mtGestureLearning">http://tinyurl.com/mtGestureLearning</a>).
I will be using AMELiA (<a href="http://ame4.hc.asu.edu/amelia/">http://ame4.hc.asu.edu/amelia/</a>) as the gesture recognition
toolkit for this project. AMELiA is a generic gesture recognition toolkit that
allows developers to build domain specific models for learning and recognizing
gestures. I will be using this toolkit to build models specifically for multitouch
input. The probabilistic bayesian models availabe within the AMELiA toolkit are
appropriate for creating Interaction Contexts with a set of gestures that the application
supports. As the library includes the training and recognition algorithms, my
project will only require implementing the domain specific code for the modeling
multitouch data. This makes the project tractable within the timeline of summer
of code.
<br><br>
This project takes it’s inspiration from the gestures developed byWayneWesterman
during for his PhD thesis that evolved into the company FingerWorks and
are now in Apple’s new multitouch trackpads. One of the key difference to note between
my proposed project and Westerman’s work is that a multitouch input-only
device was used to develop gestures for his devices, whereas today we are working
with interactive screens, that merge display and input into the same space. To use
gestures affectively, we will need to experiment with multiple mappings for operations
with different gestures. This project develops the framework for interaction
designers to easily create and use multitouch gestures within their application.
Orientation: The orientation of a multitouch gesture for a recognition algorithm
is tied to the problem of dimensionality. Since touchlib/tBeta assigns id’s
in increasing order to contacts, multiple fingers may have varying order across
samples. For example, consider a three finger gesture with the thumb, index and
middle finger. During the first sample, the fingers may have id’s (0,1,2) whereas
the second time the gesture is performed their id’s might be (1,0,2) respectively.
This is a problem for the recognizer. A heuristic that has worked to solve this
problem uses the orientation of each finger to consistently re-assign id’s to the
contacts across samples. I will apply a shape matching heuristic that can quickly
re-assign id’s by matching the shape of the contacts. This will make all samples
of a gesture have consistent id ordering, and allow the gestures to be recognized
independent of the orientation.
<br>
Development Methodologies:
My proposed project will be developed in a C++ makefile environment that can
compile cross-platform. This project will be developed through a series of iterations,
with a concrete deliverable at every stage.
<br><br></p>

<hr><p>GSoC 2009 project
<br><a href="http://www.google-melange.com/gsoc/homepage/google/gsoc2009"><img src="http://db.tt/Uai2WuQE" alt="GSoC 2009"></a>
<br><strong>Student:</strong> Sashikanth Damaraju
<br><strong>Mentor:</strong> Stjepan Rajko</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/nuigroup">nuigroup</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-38848919-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>